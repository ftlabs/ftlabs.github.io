---
layout: post
title: Training the Archive OCR with better characters
date: 2017-01-07
categories: [ex-wordpress]
permalink: /2017/01/training-the-archive-ocr-with-better-characters/
excerpt: >
  We still had whole decades of FT issues where the OCR had failed to extract any meaningful text from the scanned images of the pages.
excerpt-image: /assets/uploads/2017/01/Screen-Shot-2017-01-19-at-17.00.17-300x93.png
---
<p>After achieving a significant improvement in the accuracy of text extracted from the FT Archives (scans of every page of every issue since 1888) using just the default settings of <a href="https://github.com/tesseract-ocr" onclick="__gaTracker('send', 'event', 'outbound-article', 'https://github.com/tesseract-ocr', 'Tesseract');" target="_blank">Tesseract</a> (<a href="/2017/01/improving-text-extraction-from-the-ft-archives-with-tesseract/" target="_blank">see previous blog post</a>), we still had whole decades of FT issues where the <a href="https://en.wikipedia.org/wiki/Optical_character_recognition" onclick="__gaTracker('send', 'event', 'outbound-article', 'https://en.wikipedia.org/wiki/Optical_character_recognition', 'OCR');" target="_blank">OCR</a> had failed to extract any meaningful text from the scanned images of the pages. Since we (humans) could read the text in most of those images ok, it was obviously possible to extract meaning from them. We persevered with Tesseract, in particular its training feature.</p>
<h2>Training Tesseract</h2>
<p>Tesseract is powered by a neural network which can be fed new examples of what various alphanumeric characters look like in a variety of contexts. Each item of training data is one instance of one character, including the correct value of the character, its location in the image of the article, and the cropped image of that one character extracted from the page image.</p>
<p>But how do we get such precise coordinates and images for each and every character of every article in every page of every issue? From Tesseract. When Tesseract scans a document, it will typically dump the text it finds into a text file in one whole piece with limited effort having been made to maintain the structure of the document, but Tesseract can also be told to instead to dump out a text file with the coordinates of each character image it identified.<br />
<span id="more-14298"></span></p>
<p><a href="/assets/uploads/2017/01/Screen-Shot-2017-01-19-at-17.00.17.png"><img alt='letters captured in the wilds of the Archive' src="/assets/uploads/2017/01/Screen-Shot-2017-01-19-at-17.00.17.png" width="659" class="aligncenter size-full wp-image-14300" srcset="/assets/uploads/2017/01/Screen-Shot-2017-01-19-at-17.00.17.png 659w, /assets/uploads/2017/01/Screen-Shot-2017-01-19-at-17.00.17-300x93.png 300w" sizes="(max-width: 659px) 100vw, 659px" /></a></p>
<p>With these coordinates, we can construct the image and text files needed to train Tesseract to better understand the documents it is scanning. Problem is, we can&#8217;t train a neural network with this information directly, since the value of each character is set to whatever Tesseract thought it was during its initial OCR pass. No new information has been added, so the neural network would not be able to improve.</p>
<h2>Introducing &#8220;The Educator&#8221;</h2>
<p>The Educator is a small web app which takes the character data produced by Tesseract and asks a person whether or not a specific letter it is displaying is the letter it thinks it is. If no, they either correct it or tell us that what had been scanned was not, in fact, a letter.</p>
<p><a href="/assets/uploads/2017/01/Screen-Shot-2017-01-19-at-16.58.56.png"><img alt='The Educator asks if it is a W' src="/assets/uploads/2017/01/Screen-Shot-2017-01-19-at-16.58.56.png" width="449" class="aligncenter size-full wp-image-14299" srcset="/assets/uploads/2017/01/Screen-Shot-2017-01-19-at-16.58.56.png 449w, /assets/uploads/2017/01/Screen-Shot-2017-01-19-at-16.58.56-300x154.png 300w" sizes="(max-width: 449px) 100vw, 449px" /></a></p>
<p>We can now teach Tesseract how to read the different views of each alphanumeric character appearing in the newspaper, and we can create different sets of views for the different decades that we want to scan. </p>
<p>But, this is still a largely manual process. A person has to sit down and tell the computer what letter is what, and that can become tiresome after a while &#8211; even with tools such as the Educator that make the job a bit easier. Manually correcting 1300 characters via the Educator takes approximately an hour and that is a drop in the ocean compared to the many millions of examples found by Tesseract.</p>
<h2>Scaling up the Educator</h2>
<p>We are considering several options to to tackle this problem, the FT’s equivalent of <a href="https://techcrunch.com/2012/03/29/google-now-using-recaptcha-to-decode-street-view-addresses/" onclick="__gaTracker('send', 'event', 'outbound-article', 'https://techcrunch.com/2012/03/29/google-now-using-recaptcha-to-decode-street-view-addresses/', 'Google’s use of reCaptcha');" target="_blank">Google’s use of reCaptcha</a> to parse street signs and house numbers on Google Maps, and analysing astronomical data for signs of E.T. with <a href="https://setiathome.berkeley.edu/" onclick="__gaTracker('send', 'event', 'outbound-article', 'https://setiathome.berkeley.edu/', 'SETI@Home');" target="_blank">SETI@Home</a>.</p>
<p>In no particular order: </p>
<ul>
<li>Wait for OCR technology to improve. This seems like an increasingly viable approach, given the recent strides in Machine Learning.</li>
<li>Simply publish extracts from the archive and invite readers to respond if they spot typos</li>
<li>Offer a prize for readers finding the ‘best’ transcription errors (see the &#8216;slightly fairer&#8217; example image in the <a href="/2017/01/improving-text-extraction-from-the-ft-archives-with-tesseract/" target="_blank">previous post</a> for just such an error &#8230;)</li>
<li>Offer short-term subscriptions to readers in return for transcribing articles</li>
<li>Offer a Quid Pro Quo to readers running Ad Blockers &#8211; transcribe a few blurry words and we’ll stop nagging.</li>
<li>&#8230; and many more</li>
</ul>
<p>This and the <a href="/2017/01/improving-text-extraction-from-the-ft-archives-with-tesseract/" target="_blank">previous (archive-related) post</a> are based on notes from a <a href="https://twitter.com/MobliMic/status/797456559043411968" onclick="__gaTracker('send', 'event', 'outbound-article', 'https://twitter.com/MobliMic/status/797456559043411968', 'presentation');" target="_blank">presentation</a> given at <a href="https://twitter.com/BarcampSouth" onclick="__gaTracker('send', 'event', 'outbound-article', 'https://twitter.com/BarcampSouth', 'Barcamp Southampton');" target="_blank">Barcamp Southampton</a>.</p>
