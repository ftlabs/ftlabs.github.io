<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>FT Labs &#8211; FT Labs</title>
	<atom:link href="http://labs.ft.com/author/ftlabs/feed/" rel="self" type="application/rss+xml" />
	<link>http://labs.ft.com</link>
	<description>Labs is a team at the &#60;a href=&#34;http://www.ft.com&#34; class=&#34;ft&#34;&#62;Financial Times&#60;/a&#62; which experiments with exciting new web technologies. &#60;a href=&#34;/about&#34;&#62;Learn more&#60;/a&#62;</description>
	<lastBuildDate>Tue, 28 Mar 2017 09:47:19 +0000</lastBuildDate>
	<language>en-GB</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.6.4</generator>
	<item>
		<title>Listen to the FT</title>
		<link>http://labs.ft.com/2017/02/listen-to-the-ft/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=listen-to-the-ft</link>
		<pubDate>Thu, 23 Feb 2017 14:38:48 +0000</pubDate>
		<dc:creator><![CDATA[FT Labs]]></dc:creator>
				<category><![CDATA[Labs]]></category>

		<guid isPermaLink="false">http://labs.ft.com/?p=14316</guid>
		<description><![CDATA[A rather lovely subscriber-only, audio article player, in the form of a progressive web app.]]></description>
				<content:encoded><![CDATA[<h1>Summary</h1>
<p>We have built <a href="http://listen.ft.com" target="_blank">listen.ft.com</a>, a rather lovely subscriber-only podcast player of audio articles, in the form of a progressive web app. It’s a prototype, but it works well, and taps into an oft-stated need.</p>
<p><figure><img alt='Screen Shot of listen.ft.com' src="http://labs.ft.com/wp-content/uploads/2017/02/screen-shot-2-179x300.png" width="179" class="alignleft size-medium wp-image-14324" srcset="http://labs.ft.com/wp-content/uploads/2017/02/screen-shot-2-179x300.png 179w, http://labs.ft.com/wp-content/uploads/2017/02/screen-shot-2.png 573w" sizes="(max-width: 179px) 100vw, 179px" /></figure></p>
<h1>Details</h1>
<h2>What was the problem?</h2>
<p>Part of FT Labs’ remit is to engage with the business development team to explore new, interesting ways to enhance the FT&#8217;s offering to our readers.</p>
<p>Over the last few months, we’ve been exploring offering audio versions of FT content to subscribers &#8211; the newspaper in podcast form, if you will. Initially, this took the form of inserting an audio player with a spoken word version of the content into the corresponding article page, which is great at trying to gauge the interest of readers in this kind of content, but it’s very MVP.</p>
<p>There are better ways to consume audio content. </p>
<h2>So what did we do about it?</h2>
<p>With quite a few audio articles available, we took on the challenge to make listening to the FT both easy, and a joy. Our solution was a progressive web app, <a href="http://listen.ft.com" target="_blank">listen.ft.com</a> (subscribers only).</p>
<p>A progressive web app, is a web page like any other, except that it’s been _progressively_ enhanced to take advantage of the latest web APIs and capabilities. This tends to mean that it can do a little bit more than a typical web page (approaching what would previously have required installed an app from the app store), but it will also fall back to older, better supported web technologies for browsers *cough* Mobile Safari *cough*.</p>
<p>The particular ability we were after for this project was for it to work offline, since trains do go through tunnels. </p>
<p><figure><img alt='listen_screenshot_1' src="http://labs.ft.com/wp-content/uploads/2017/02/listen_screenshot_1.png" width="1152" class="alignleft size-full wp-image-14323" srcset="http://labs.ft.com/wp-content/uploads/2017/02/listen_screenshot_1.png 1152w, http://labs.ft.com/wp-content/uploads/2017/02/listen_screenshot_1-300x71.png 300w, http://labs.ft.com/wp-content/uploads/2017/02/listen_screenshot_1-768x181.png 768w, http://labs.ft.com/wp-content/uploads/2017/02/listen_screenshot_1-1024x242.png 1024w" sizes="(max-width: 1152px) 100vw, 1152px" /></figure></p>
<p>Also, if used frequently, the <a href="http://listen.ft.com" target="_blank">listen.ft.com</a> page can be installed to an (Android) user’s home screen.</p>
<h2>Which FT content can I listen to?</h2>
<p>We aren’t (yet) making the entirety of the FT&#8217;s content available in audio form. Creating human-read audio versions of a written article is a time-consuming (and expensive) process, and we produce ~300 articles a day! Not only are we unable to produce that volume, we don’t think many people will be able to consume that volume, either. </p>
<p>As such, content that will be available on <a href="http://listen.ft.com" target="_blank">listen.ft.com</a> will most likely either be long-form or ‘evergreen’ content. These pieces represent some of the best of FT content and insights, so we want to put it in front of as many people in as many ways as possible.</p>
<p>As the number of audio articles increases, <a href="http://www.ft.com/myft" target="_blank">myFT</a> will give the user control over which topics should appear in the listen app.</p>
<h2>Can’t I just listen to Podcasts?</h2>
<p>Consuming content through audio isn’t especially new &#8211; podcasts are enjoying a resurgence in popularity, and the ecosystem work really well, but they’re an awkward proposition for a business that deals in selling its content. Releasing a news article into the world in a podcast format means exposing it from behind our paywall, and destroying any ability we have to control (and track) how widely that news piece is consumed.</p>
<p>As a prototype, we have not attempted to create the definitive audio player &#8211; it is clear a great deal more UX-thinking is needed. That said, it is lovely (and you can select the play speed of the audio &#8211; our one concession to feature creep).</p>
<p><figure><img alt='you can adjust the play speed' src="http://labs.ft.com/wp-content/uploads/2017/02/square_img.png" width="752" class="alignleft size-full wp-image-14325" srcset="http://labs.ft.com/wp-content/uploads/2017/02/square_img.png 752w, http://labs.ft.com/wp-content/uploads/2017/02/square_img-273x300.png 273w" sizes="(max-width: 752px) 100vw, 752px" /></figure></p>
<p>Longer term, we will be tackling more, much richer, voice-mediated interactions with FT.com, and <a href="http://listen.ft.com" target="_blank">listen.ft.com</a> gives us a good platform on which to work.</p>
<h2>Technical difficulties experienced along the way</h2>
<p>(which may be worthy of followup posts) </p>
<ul>
<li>Keeping the audio behind a paywall, so not simply exposing the audio as a podcast feed, but also making the whole experience frictionless</li>
<li>Working offline</li>
<li>Knowing when online was in fact offline</li>
<li>Tracking user interactions when offline (e.g. which articles are played all the way through)</li>
<li>Making it work on an iphone browser (the new IE6)</li>
<li>Navigating the challenging documentation on Service Workers</li>
</ul>
]]></content:encoded>
			</item>
		<item>
		<title>Ad Blocking Experiments</title>
		<link>http://labs.ft.com/2017/02/ad-blocking-experiments/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=ad-blocking-experiments</link>
		<pubDate>Thu, 23 Feb 2017 11:49:45 +0000</pubDate>
		<dc:creator><![CDATA[FT Labs]]></dc:creator>
				<category><![CDATA[Labs]]></category>

		<guid isPermaLink="false">http://labs.ft.com/?p=14312</guid>
		<description><![CDATA[In an interesting coming-together of serious and whimsical, FT Labs worked on some Ad Blocking ideas to help the Ads team address this thorny issue on FT.com. The prototypes we built helped trigger some excellent internal debate, teasing out the issues and subtleties of a very thorny topic.]]></description>
				<content:encoded><![CDATA[<h2>Summary</h2>
<p>In an interesting coming-together of serious and whimsical, FT Labs worked on some Ad Blocking ideas to help the Ads team address this thorny issue on FT.com. The prototypes we built helped trigger some excellent internal debate, teasing out many of the issues and subtleties. One of the prototypes (‘missing words’) formed part of a multivariate test on Registered (non subs-paying) users. The media response to the test was generally positive. Sadly, the outcome of the test leads us to think that a simple block on displaying article content, when a user is Ad Blocking, leads to the better outcome. But the debates were good&#8230;</p>
<div style="display: block; position: relative; max-width: 100%;">
<div style="padding-top: 56.25%;"><iframe src="//players.brightcove.net/452318443/default_default/index.html?videoId=5333859149001" 
allowfullscreen 
webkitallowfullscreen 
mozallowfullscreen 
style="width: 100%; height: 100%; position: absolute; top: 0px; bottom: 0px; right: 0px; left: 0px;"></iframe></div>
</div>
<p><span id="more-14312"></span></p>
<h1>Details</h1>
<h2>Ads and Ad Blocking</h2>
<p>For decades, advertising has been a major source of revenue for news organisations and publishers around the world. Recently, revenue from printed advertising has declined sharply and, for a variety of reasons, advertising from online sources hasn&#8217;t been able to make up the difference. To add to this, users are now able to easily block ads from their web experience. We understand people don&#8217;t want to be followed and hounded across the web, but we&#8217;re not like that. Ultimately, good quality journalism, such as the FT offers, needs revenue to allow us to continue the work we do and, since the FT operates a mixed revenue model, some of that revenue comes from advertising to our users.</p>
<h2>What can we do about Ad Blocking?</h2>
<p>Ultimately, a suite of ad-block solutions was built and deployed to a cohort of FT.com users (registered, but not paying subs), based on the much larger set of candidate solutions dreamed up in FT Labs and the wider business.</p>
<p>If we detected ad-blocking on our site, we would do one of the following</p>
<ol>
<li>Remove a number of words from the article that roughly represented a percentage value of the amount of revenue the FT loses to ad-blockers (with an explanation of why&#8230;). </li>
<li>Explain about the problems caused by Ad blocking and request that the users whitelist the site, but leave the article viewable</li>
<li>Simply block access to the article whilst ad blocking was in place</li>
<li>Do nothing</li>
</ol>
<p>It was felt by Labs and the Ads team that this idea was a good middle ground between punishing our users, and letting them continue with ad-blocking unchecked, but some of the other ideas we had along the way were a pleasure to explore, safe in the knowledge that they would <strong>never</strong> see the light of day. </p>
<h2>Ad Blocking Experiments</h2>
<p>The Labs department was asked to partner with the Ads team and find creative, effective solutions to our ad-blocking problem. We aren&#8217;t the first publisher to have a look at solving this problem. In recent times, publishers have created myriad solutions that aim to coerce or convince users into adding their websites into the whitelist of their ad-blockers, or disabling them altogether. They have been humourous, blunt, subtle, and some have ended up drawing the ire of social media users. Here are few instances we found interesting, enjoyed, and/or had a measurable effect on the number of users ad-blocking.</p>
<ul>
<li><a href="http://digiday.com/uk/le-figaro-got-20-percent-ad-blocking-readers-whitelist-site/" target="_blank">http://digiday.com/uk/le-figaro-got-20-percent-ad-blocking-readers-whitelist-site/</a></li>
<li><a href="https://www.wired.com/2016/08/no-one-can-stop-ad-blocking-not-even-facebook/" target="_blank">https://www.wired.com/2016/08/no-one-can-stop-ad-blocking-not-even-facebook/</a></li>
</ul>
<p>So what could we do that was different? Initially, the Labs and Ads teams sat together and had some brainstorming sessions in which we came up with a handful of ideas, and afterwards we elicited ideas from other members of staff. </p>
<p>We prototyped these ideas on the then beta version of the FT.com and deployed them behind a flag, so that only FT staff members could see the demos. Once the our demos had been enabled, a UI would appear at the top of each article page that would allow the user to flick through and view all of the ideas we had had.</p>
<p><figure><img alt='Screen Shot 2017-02-22 at 15.36.22' src="http://labs.ft.com/wp-content/uploads/2017/02/Screen-Shot-2017-02-22-at-15.36.22.png" width="1504" class="alignleft size-full wp-image-14314" srcset="http://labs.ft.com/wp-content/uploads/2017/02/Screen-Shot-2017-02-22-at-15.36.22.png 1504w, http://labs.ft.com/wp-content/uploads/2017/02/Screen-Shot-2017-02-22-at-15.36.22-300x119.png 300w, http://labs.ft.com/wp-content/uploads/2017/02/Screen-Shot-2017-02-22-at-15.36.22-768x305.png 768w, http://labs.ft.com/wp-content/uploads/2017/02/Screen-Shot-2017-02-22-at-15.36.22-1024x407.png 1024w" sizes="(max-width: 1504px) 100vw, 1504px" /></figure></p>
<p>Broadly, our ideas fell into one or more of these categories:</p>
<ol>
<li>Punish our users </li>
<p>     No ads, no whitelist, no service.</p>
<li>Amuse our users&#8230;</li>
<p>    So they look more charitably upon us.</p>
<li>Utilise our users. </li>
<p>    If we can&#8217;t get revenue from them, can we make them work for us?</p>
<li>Explanation</li>
<p>    Level with our users and let them know why ads are important</p>
<li>Passive aggressive</li>
<p>    We&#8217;re not mad that you&#8217;re blocking ads, we&#8217;re just disappointed
</ol>
<p>We did not worry too much, at this stage, about the risk of offending users, since we were mainly interested in provoking internal discussions around Ad Blocking, and exploring the possibilities. </p>
<p>It should be noted that the following were considered as experiments only, and most were never intended for public consumption.</p>
<ol>
<li>No Vowels</li>
<p>This was our first idea. If we detected a user was blocking ads, we would punish them by removing all of the vowels in the article, making it difficult to read. We thought we were being really creative with this, but it turns out to be a common idea over the years that’s come to be known as ‘<a href="http://www.geekwire.com/2016/revenge-readers-jeff-bezos-proposed-pay-disemvowel-feature-washington-post-stories/" target="_blank">Disemvoweling</a>’</p>
<li>Bart</li>
<p>When Bart Simpson misbehaves, he’s punished by being made to write out the things he should not do over and over again on a blackboard. Much in the same guise, when an ad-blocker tries to view an article, we overwrite said article with the thing they should not do &#8211; I will not block ads.</p>
<li>Revenue</li>
<p>Remove an amount of words from each article roughly representative of the revenue lost from blocked ads.</p>
<li>Questions</li>
<p>We would ask users what reasons they had for blocking ads, and responding to answers with counterarguments.</p>
<li>Ad Articles</li>
<p>The most passive-aggressive proposal in this list. Between every second and third paragraph in an article that had its ads blocked, a modal was inserted that linked to an FT article about ad-blocking. </p>
<li>Delay Page</li>
<p>The longer ads were blocked on our site, the longer we would make the user wait to view each article. Our reasoning was that eventually, user frustration with the delay would outweigh any perceived benefits of blocking the ads.</p>
<li>Ad Guidelines</li>
<p>A simple modal dialog that tried to educate our users about why our adverts are better/more responsibly sourced than adverts from other publishers</p>
<li>Word Shuffle</li>
<p>Much like No Ads, No Vowels, we would adjust the content of the article, but instead of removing characters we would rearrange them into an almost intelligible gibberish.</p>
<li>Archive OCR</li>
<p>If we can’t get users to view ads, maybe we can benefit from their labour instead. In order to read the article with an adblocker, a user would have to transcribe an image of an unidentified phrase fragment from our historical archives.
</ol>
<p>As part of the demos, a small questionnaire was included for our colleagues to give us feedback on how they felt users would respond to each idea, and any ideas of their own that they may have.</p>
<p>After giving FT staff two weeks to play with and mull over the options we&#8217;d presented to them, and having received feedback on all of those demos, the Ads team selected and deployed one of the ideas as part of an A/B test to measure the effect. Ultimately, we decided to go ahead and test the revenues idea on a portion of our ad-blocking users ad revenues idea. Now, if we detected ad-blocking on our site, we would remove a number of words from the article that roughly represented a percentage value of the amount of revenue the FT loses to ad-blockers. </p>
<p>The users we decided to test our approach on we classify as &#8216;registered&#8217; users. These are users who have an FT.com account, but have never had payment details or a subscription added to that account. The only revenue we gain from these users is through advertising, so if they&#8217;re blocking ads, we need to address that. </p>
<p>We were also very keen to not experiment on our paying customers with a potentially highly contentious experience. </p>
<h2>Results</h2>
<p>The experiment ran for 30 days, and the media response was generally positive.<br />
e.g. </p>
<ul>
<li><a href="http://digiday.com/uk/financial-times-cracking-ad-blocking-metaphor/" target="_blank">http://digiday.com/uk/financial-times-cracking-ad-blocking-metaphor/</a></li>
<li><a href="http://adage.com/article/media/financial-times-decides-creative-ad-blocking/305040/" target="_blank">http://adage.com/article/media/financial-times-decides-creative-ad-blocking/305040/</a></li>
<li><a href="https://twitter.com/dmedialab/status/755828734330941440" target="_blank">https://twitter.com/dmedialab/status/755828734330941440</a></li>
<li><a href="https://twitter.com/bradbox/status/756468447912026112" target="_blank">https://twitter.com/bradbox/status/756468447912026112</a></li>
<li><a href="https://twitter.com/Adsperity/status/756141523830464512" target="_blank">https://twitter.com/Adsperity/status/756141523830464512</a></li>
</ul>
<p>Once the A/B test had run its course, the results were as follows:</p>
<p>When presented with our message and word removal, 46.59% of users opted to whitelist or disable their ad-blockers to view the FT.</p>
<p>Our page view actually increased by 9.41% for the period of time that our test was in place. Though not statistically significant, it certainly is interesting. We believe people clicking on extra pages to make sure the words had returned may be the cause of this result.</p>
<p>Subsequent visits to the site were unaffected, so asking people to disable their ad blockers hasn’t had a negative effect.</p>
<p>We&#8217;ve established via longer term testing that simply blocking access (for registered but not subscribed users), with an explanation of why, leads to a higher proportion of users actually whitelisting FT.com and reading more articles.</p>
<p>It is with some sadness that we will park Bart and his ilk in this blog.</p>
]]></content:encoded>
			</item>
		<item>
		<title>Training the Archive OCR with better characters</title>
		<link>http://labs.ft.com/2017/01/training-the-archive-ocr-with-better-characters/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=training-the-archive-ocr-with-better-characters</link>
		<pubDate>Thu, 19 Jan 2017 17:08:44 +0000</pubDate>
		<dc:creator><![CDATA[FT Labs]]></dc:creator>
				<category><![CDATA[FT Archive]]></category>
		<category><![CDATA[Labs]]></category>

		<guid isPermaLink="false">http://labs.ft.com/?p=14298</guid>
		<description><![CDATA[After achieving a significant improvement in the accuracy of text extracted from the FT Archives (scans of every page of every issue since 1888) using just the default settings of Tesseract (see previous blog post), we still had whole decades &#8230;]]></description>
				<content:encoded><![CDATA[<p>After achieving a significant improvement in the accuracy of text extracted from the FT Archives (scans of every page of every issue since 1888) using just the default settings of <a href="https://github.com/tesseract-ocr" target="_blank">Tesseract</a> (<a href="http://labs.ft.com/2017/01/improving-text-extraction-from-the-ft-archives-with-tesseract/" target="_blank">see previous blog post</a>), we still had whole decades of FT issues where the <a href="https://en.wikipedia.org/wiki/Optical_character_recognition" target="_blank">OCR</a> had failed to extract any meaningful text from the scanned images of the pages. Since we (humans) could read the text in most of those images ok, it was obviously possible to extract meaning from them. We persevered with Tesseract, in particular its training feature.</p>
<h2>Training Tesseract</h2>
<p>Tesseract is powered by a neural network which can be fed new examples of what various alphanumeric characters look like in a variety of contexts. Each item of training data is one instance of one character, including the correct value of the character, its location in the image of the article, and the cropped image of that one character extracted from the page image.</p>
<p>But how do we get such precise coordinates and images for each and every character of every article in every page of every issue? From Tesseract. When Tesseract scans a document, it will typically dump the text it finds into a text file in one whole piece with limited effort having been made to maintain the structure of the document, but Tesseract can also be told to instead to dump out a text file with the coordinates of each character image it identified.<br />
<span id="more-14298"></span></p>
<p><a href="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-19-at-17.00.17.png"><img alt='letters captured in the wilds of the Archive' src="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-19-at-17.00.17.png" width="659" class="aligncenter size-full wp-image-14300" srcset="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-19-at-17.00.17.png 659w, http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-19-at-17.00.17-300x93.png 300w" sizes="(max-width: 659px) 100vw, 659px" /></a></p>
<p>With these coordinates, we can construct the image and text files needed to train Tesseract to better understand the documents it is scanning. Problem is, we can&#8217;t train a neural network with this information directly, since the value of each character is set to whatever Tesseract thought it was during its initial OCR pass. No new information has been added, so the neural network would not be able to improve.</p>
<h2>Introducing &#8220;The Educator&#8221;</h2>
<p>The Educator is a small web app which takes the character data produced by Tesseract and asks a person whether or not a specific letter it is displaying is the letter it thinks it is. If no, they either correct it or tell us that what had been scanned was not, in fact, a letter.</p>
<p><a href="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-19-at-16.58.56.png"><img alt='The Educator asks if it is a W' src="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-19-at-16.58.56.png" width="449" class="aligncenter size-full wp-image-14299" srcset="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-19-at-16.58.56.png 449w, http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-19-at-16.58.56-300x154.png 300w" sizes="(max-width: 449px) 100vw, 449px" /></a></p>
<p>We can now teach Tesseract how to read the different views of each alphanumeric character appearing in the newspaper, and we can create different sets of views for the different decades that we want to scan. </p>
<p>But, this is still a largely manual process. A person has to sit down and tell the computer what letter is what, and that can become tiresome after a while &#8211; even with tools such as the Educator that make the job a bit easier. Manually correcting 1300 characters via the Educator takes approximately an hour and that is a drop in the ocean compared to the many millions of examples found by Tesseract.</p>
<h2>Scaling up the Educator</h2>
<p>We are considering several options to to tackle this problem, the FT’s equivalent of <a href="https://techcrunch.com/2012/03/29/google-now-using-recaptcha-to-decode-street-view-addresses/" target="_blank">Google’s use of reCaptcha</a> to parse street signs and house numbers on Google Maps, and analysing astronomical data for signs of E.T. with <a href="https://setiathome.berkeley.edu/" target="_blank">SETI@Home</a>.</p>
<p>In no particular order: </p>
<ul>
<li>Wait for OCR technology to improve. This seems like an increasingly viable approach, given the recent strides in Machine Learning.</li>
<li>Simply publish extracts from the archive and invite readers to respond if they spot typos</li>
<li>Offer a prize for readers finding the ‘best’ transcription errors (see the &#8216;slightly fairer&#8217; example image in the <a href="http://labs.ft.com/2017/01/improving-text-extraction-from-the-ft-archives-with-tesseract/" target="_blank">previous post</a> for just such an error &#8230;)</li>
<li>Offer short-term subscriptions to readers in return for transcribing articles</li>
<li>Offer a Quid Pro Quo to readers running Ad Blockers &#8211; transcribe a few blurry words and we’ll stop nagging.</li>
<li>&#8230; and many more</li>
</ul>
<p>This and the <a href="http://labs.ft.com/2017/01/improving-text-extraction-from-the-ft-archives-with-tesseract/" target="_blank">previous (archive-related) post</a> are based on notes from a <a href="https://twitter.com/MobliMic/status/797456559043411968" target="_blank">presentation</a> given at <a href="https://twitter.com/BarcampSouth" target="_blank">Barcamp Southampton</a>.</p>
]]></content:encoded>
			</item>
		<item>
		<title>Improving text extraction from the FT Archives with Tesseract</title>
		<link>http://labs.ft.com/2017/01/improving-text-extraction-from-the-ft-archives-with-tesseract/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=improving-text-extraction-from-the-ft-archives-with-tesseract</link>
		<pubDate>Thu, 19 Jan 2017 11:45:42 +0000</pubDate>
		<dc:creator><![CDATA[FT Labs]]></dc:creator>
				<category><![CDATA[FT Archive]]></category>
		<category><![CDATA[Labs]]></category>

		<guid isPermaLink="false">http://labs.ft.com/?p=14291</guid>
		<description><![CDATA[Good quality OCR for very little effort.]]></description>
				<content:encoded><![CDATA[<p>We have an online archive, currently only available in-house, of all printed issues of the Financial Times newspaper, from the first issue in <a href="http://aboutus.ft.com/files/2010/09/FThistory.jpg" target="_blank">1888 through to 2010</a>. Each page of each issue has been photographed, divided into distinct articles, and each article has been processed with OCR (<a href="https://en.wikipedia.org/wiki/Optical_character_recognition" target="_blank">Optical Character Recognition</a>) to extract the source text from the image. </p>
<p>The archive weighs in at approximately 2 TB of JPGs (one for each page of each issue) and XML (one monster file per issue, containing the positional data and titles of each article, and the text fragments identified within each article).</p>
<p>As we consider how to integrate the content of our archive into the main site, <a href="https://open.blogs.nytimes.com/2016/07/26/the-future-of-the-past-modernizing-the-new-york-times-archive" target="_blank">as others have done</a>, one significant step is to improve the OCR: the extracted text varies from almost perfect transcription to random noise.</p>
<p>We used open-source software, <a href="https://github.com/tesseract-ocr" target="_blank">Tesseract</a>, with its default settings, to achieve a very gratifying improvement in OCR quality.<br />
<span id="more-14291"></span></p>
<p><a href="http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR1.png"><img alt='Image of article - original OCR scan - Tesseract default scan ' src="http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR1.png" width="2000" class="size-full wp-image-14293" srcset="http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR1.png 2000w, http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR1-300x180.png 300w, http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR1-768x461.png 768w, http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR1-1024x614.png 1024w" sizes="(max-width: 2000px) 100vw, 2000px" /></a></p>
<p>On the left, we have an article which has been cropped from its parent page according to the coordinates stored in the XML. In the middle, we have the original OCR text that we received from the 3rd party. And on the right we have the results of a default OCR scan from Tesseract. It&#8217;s not perfect, but it&#8217;s rather good for an off-the-shelf piece of kit. And what&#8217;s even more impressive is that Tesseract correctly identified a 55 year old spelling error!</p>
<p>We continued with Tesseract to make further OCR improvements, described in a later post, but the remainder of this post describes how we used the out-of-the-box version to make the first big improvement. These posts are based on notes from a <a href="https://twitter.com/MobliMic/status/797456559043411968" target="_blank">presentation</a> given at <a href="https://twitter.com/BarcampSouth" target="_blank">Barcamp Southampton</a>.</p>
<h2>The problem: inconsistent OCR quality</h2>
<p>The problem we have here, is that the archived page image quality has changed over time. The settings that create the ideal circumstance for OCR aren&#8217;t constant across the entirety of our dataset. This is unfortunate, but the 3rd party company we engaged was hardly going to manually dial in the OCR settings for each and every one of the individual issues.</p>
<p>One thing to note is that although the general quality of the page images varies across the century, each decade is largely consistent within itself. The clarity and level of detail in the images from a January 1960 edition is roughly equivalent to the images from a December 1969 edition. That said, the images are quite noisy, often misaligned, sometimes with movement-induced discontinuities across the middle of a page. However, the bigger problem isn&#8217;t so much <em>&#8220;Can we scan the entire page or entire issue in one pass&#8221;</em> but more <em>&#8220;Can we scan decades of issues without too much configuration and with a reasonable result. Also we&#8217;ve changed fonts across the decades! Oh, the paper, ink, and printing machinery also varies.&#8221;</em></p>
<h2>The opportunity: good positional data and Tesseract</h2>
<p>The initial OCR content was not particularly high quality, BUT the data in the XML files describing the location of that content was both comprehensive and accurate. This meant that we could use the XML files to re-run OCR on the images on an article-by-article basis.</p>
<figure id="attachment_14288" class="wp-caption aligncenter"><a href="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-18-at-15.38.01.png"><img alt='individual page components and articles highlighted, based on the XML data' src="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-18-at-15.38.01.png" width="1420" class="size-full wp-image-14288" srcset="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-18-at-15.38.01.png 1420w, http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-18-at-15.38.01-300x215.png 300w, http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-18-at-15.38.01-768x551.png 768w, http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-18-at-15.38.01-1024x734.png 1024w" sizes="(max-width: 1420px) 100vw, 1420px" /></a><figcaption class="wp-caption-text">individual page components and articles highlighted, based on the XML data</figcaption></figure>
<p>Plus, there was Tesseract, a piece of open-source software that&#8217;s really good at one thing &#8211; OCR scanning.</p>
<p>We picked a few articles at random from across the decades and ran Tesseract with its default settings, and got the spectacular results above.</p>
<p>To be slightly fairer to the original provider of our OCR, here is another, more typical improvement in OCR</p>
<p><a href="http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR2.png"><img alt='DarkOCR2' src="http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR2.png" width="2040" class="aligncenter size-full wp-image-14292" srcset="http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR2.png 2040w, http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR2-300x176.png 300w, http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR2-768x452.png 768w, http://labs.ft.com/wp-content/uploads/2017/01/DarkOCR2-1024x602.png 1024w" sizes="(max-width: 2040px) 100vw, 2040px" /></a></p>
<p>The improvement brought about by using Tesseract is sufficient to justify re-doing the OCR across the entire archive but, even so, there are still sections of the archive which remain illegible (for our OCR). </p>
<h2>Gotchas</h2>
<p>There are several challenges we noted but parked for the duration of this project</p>
<ul>
<li>Combining articles which are split across columns or pages</li>
<li>Identifying adverts (rather than journalistic copy)</li>
<li>Handling tabular text</li>
</ul>
]]></content:encoded>
			</item>
		<item>
		<title>Animating 123 years of FT banners</title>
		<link>http://labs.ft.com/2017/01/animating-123-years-of-ft-banners/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=animating-123-years-of-ft-banners</link>
		<pubDate>Mon, 16 Jan 2017 15:17:56 +0000</pubDate>
		<dc:creator><![CDATA[FT Labs]]></dc:creator>
				<category><![CDATA[FT Archive]]></category>
		<category><![CDATA[Labs]]></category>

		<guid isPermaLink="false">http://labs.ft.com/?p=14280</guid>
		<description><![CDATA[From our archives, 1m31s of changing prices, fonts, layouts, and ads]]></description>
				<content:encoded><![CDATA[<p>We have an online archive, currently only available in-house, of all printed issues of the Financial Times newspaper, from the first issue in 1888 through to 2010.</p>
<p><a href="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-16-at-16.46.16.png"><img alt='the first FT banner' src="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-16-at-16.46.16-1024x217.png" width="584" class="aligncenter size-large wp-image-14286" srcset="http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-16-at-16.46.16-1024x217.png 1024w, http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-16-at-16.46.16-300x63.png 300w, http://labs.ft.com/wp-content/uploads/2017/01/Screen-Shot-2017-01-16-at-16.46.16-768x162.png 768w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>Each page of each issue has been scanned, divided into distinct articles, and each article has been processed with OCR (<a href="https://en.wikipedia.org/wiki/Optical_character_recognition" target="_blank">Optical Character Recognition</a>) to extract the source text from the image in monster XML files. As we consider how to integrate our archive into the main site, as <a href="https://open.blogs.nytimes.com/2016/07/26/the-future-of-the-past-modernizing-the-new-york-times-archive" target="_blank">others have done</a>, one significant step will be to improve the OCR, which varies from random noise to almost perfect transcription &#8211; see later blog posts.</p>
<p>Along the way, however, there was an opportunity to create an animation from the main banner images.</p>
<p><!-- [video width="1080" height="608" mp4="http://labs.ft.com/wp-content/uploads/2017/01/2016_07_28_1612_55_xml_ff_r10_size1080_padheight608.mp4"][/video] --></p>
<div style="display: block; position: relative; max-width: 100%;">
<div style="padding-top: 56.25%;"><iframe src="//players.brightcove.net/452318443/ryLbEHLe_default/index.html?videoId=5284999452001" 
allowfullscreen 
webkitallowfullscreen 
mozallowfullscreen 
style="width: 100%; height: 100%; position: absolute; top: 0px; bottom: 0px; right: 0px; left: 0px;"></iframe></div>
</div>
<p><span id="more-14280"></span></p>
<p>At this point, our editorial department leaned forward, captivated by the little history lesson playing out before them. For those of us not quite so up on the finer points of font and layout and inter-departmental office politics, the flow of prices and the longevity of the ads provides some interest.</p>
<h2>How?</h2>
<p>Along with the data on each archive article, we have the bounding box for each image on the page, so it is fairly simple to identify and extract the banner from the front page of each issue in the XML. To keep the data volume down, we restricted this to just the issue from the 1st of each month, as well as ignoring Saturdays (which have a distinctly different banner).</p>
<p>We used ImageMagick’s convert to crop the banner images (leaving some margin because the original scans were quite erratic in how the page had been placed on the scanner). We used FFmpeg to stitch together the multiple banners into a video (MP4) file, as well to scale it and add black padding. NB, the cropped image size needed to be an even number of pixels along each side for FFmpeg, and we chose a frame rate of 10 as a compromise of speed vs jitter (as mentioned, the scans were not very consistent).</p>
<p>You can see the how convert and FFmpeg were configured and used in <a href="https://github.com/ftlabs/archive-banners/blob/master/grab_and_crop.sh" target="_blank">this script</a>. </p>
<p>But wait, there’s more.</p>
<h2>123 years of “&#8230;”</h2>
<p>Since we had the location of every in-article word included in the archive XML, it was possible to create animations of every occurrence of a specific word (where OCR had managed to catch it correctly), using the same processing as above. </p>
<p>Here is one such animation, a little homage to <a href="http://spritzinc.com/" target="_blank">Spritz</a>: </p>
<p><!-- [video width="600" height="600" mp4="http://labs.ft.com/wp-content/uploads/2017/01/2016_07_29_1203_18_to_word_Europe_ff_r10.mp4"][/video] --> </p>
<div style="display: block; position: relative; max-width: 100%;">
<div style="padding-top: 56.25%;"><iframe src="//players.brightcove.net/452318443/ryLbEHLe_default/index.html?videoId=5285002154001" 
allowfullscreen 
webkitallowfullscreen 
mozallowfullscreen 
style="width: 100%; height: 100%; position: absolute; top: 0px; bottom: 0px; right: 0px; left: 0px;"></iframe></div>
</div>
<p><BR></p>
<p>Which word is in focus?</p>
]]></content:encoded>
	<enclosure url="http://labs.ft.com/wp-content/uploads/2017/01/2016_07_28_1612_55_xml_ff_r10_size1080_padheight608.mp4" length="22190680" type="video/mp4" />
<enclosure url="http://labs.ft.com/wp-content/uploads/2017/01/2016_07_29_1203_18_to_word_Europe_ff_r10.mp4" length="9360150" type="video/mp4" />
		</item>
		<item>
		<title>Recently Interesting Links – September 2016</title>
		<link>http://labs.ft.com/2016/09/recently-interesting-links-september-2016/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=recently-interesting-links-september-2016</link>
		<pubDate>Tue, 27 Sep 2016 08:30:29 +0000</pubDate>
		<dc:creator><![CDATA[FT Labs]]></dc:creator>
				<category><![CDATA[InterestingLinks]]></category>
		<category><![CDATA[Labs]]></category>

		<guid isPermaLink="false">http://labs.ft.com/?p=14271</guid>
		<description><![CDATA[A varied and inconsistent list of recently interesting items (for some definition of recent, and some definition of interesting): AI / Machine Learning &#8220;machine intelligence will sit between our actions and the world&#8221;[http://motherboard.vice.com/en_uk/read/the-arrival-of-artificially-intelligent-beer] &#8220;Enhance that image (just like in the &#8230;]]></description>
				<content:encoded><![CDATA[<p><em>A varied and inconsistent list of recently interesting items (for some definition of recent, and some definition of interesting)</em>:</p>
<p><figure><img alt='chatbots are coming your way' src="http://labs.ft.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-27-at-09.06.35.png" width="478" class="alignleft size-full wp-image-14272" srcset="http://labs.ft.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-27-at-09.06.35.png 478w, http://labs.ft.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-27-at-09.06.35-300x211.png 300w" sizes="(max-width: 478px) 100vw, 478px" /></figure></p>
<h2>AI / Machine Learning</h2>
<p><em>&#8220;machine intelligence will sit between our actions and the world&#8221;</em><BR>[<a href="http://motherboard.vice.com/en_uk/read/the-arrival-of-artificially-intelligent-beer" target="_blank">http://motherboard.vice.com/en_uk/read/the-arrival-of-artificially-intelligent-beer</a>]</p>
<p>&#8220;Enhance that image (just like in the movies)&#8221; <BR>[<a href="https://twitter.com/_iest/status/771615918141034496" target="_blank">https://twitter.com/_iest/status/771615918141034496</a>]</p>
<p>DILBERT on text summarisation &#8211; <em>&#8220;I don&#8217;t have time to read your long email. Tell me what it said.&#8221;</em> <BR>[<a href="http://dilbert.com/strip/2016-09-01" target="_blank">http://dilbert.com/strip/2016-09-01</a>] and, related, [<a href="http://dilbert.com/strip/2016-08-31" target="_blank">http://dilbert.com/strip/2016-08-31</a>]</p>
<h2>Tensor Flow</h2>
<p><em>&#8220;The Google Brain Team is open-sourcing #TensorFlow model code for summarization research&#8221;</em><BR>[<a href="https://twitter.com/deeplearningldn/status/768557870203936771" target="_blank">https://twitter.com/deeplearningldn/status/768557870203936771</a>]</p>
<p>Udacity&#8217;s TensorFlow 101: [<a href="https://www.udacity.com/course/deep-learning--ud730" target="_blank">https://www.udacity.com/course/deep-learning&#8211;ud730</a>]</p>
<p>Stanford&#8217;s TensorFlow 101: [<a href="https://twitter.com/math_rachel/status/775764096427765760" target="_blank">https://twitter.com/math_rachel/status/775764096427765760</a>]</p>
<p><em>&#8220;We show that WaveNets are able to generate speech which mimics any human voice and which sounds more natural than the best existing Text-to-Speech systems, reducing the gap with human performance by over 50%.&#8221;</em><BR>[<a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a>]</p>
<h2>Chatbots</h2>
<p><em>&#8220;chatbots have to compete with Google and, so far, it does not seem as if they have the same great potential&#8221;</em><BR>[<a href="https://www.ft.com/content/35ce58be-6920-11e6-a0b1-d87a9fea034f" target="_blank">https://www.ft.com/content/35ce58be-6920-11e6-a0b1-d87a9fea034f</a> &#8211; FT article]</p>
<p><em>&#8220;Their reactions showed that people may not be ready for bots that are too human-like.&#8221;</em><BR>[<a href="http://digiday.com/publishers/guardian-learned-chatbots/" target="_blank">http://digiday.com/publishers/guardian-learned-chatbots/</a>]</p>
<p>The lesson here appears to be more that if you get the niche right, people will be happy to sign up to very specific topics via the chat channel.<BR>[<a href="http://digiday.com/publishers/suns-football-chat-bot-drove-nearly-half-users-back-site/" target="_blank">http://digiday.com/publishers/suns-football-chat-bot-drove-nearly-half-users-back-site/</a>]</p>
<h2>Learning &#8216;too&#8217; well, or not well enough</h2>
<p><em>&#8220;If AI learns language sufficiently well, it will also learn cultural associations that are offensive, objectionable, or harmful. At a high level, bias is meaning. “Debiasing” these machine models, while intriguing and technically interesting, necessarily harms meaning.&#8221;</em><BR>[<a href="https://freedom-to-tinker.com/blog/randomwalker/language-necessarily-contains-human-biases-and-so-will-machines-trained-on-language-corpora/
" target="_blank">https://freedom-to-tinker.com/blog/randomwalker/language-necessarily-contains-human-biases-and-so-will-machines-trained-on-language-corpora/</a>]</p>
<p>First there was HITL (human in the loop), then there was SITL (society in the loop): <em>&#8220;we still lack mechanisms for articulating societal expectations (e.g. ethics, norms, legal principles) in ways that machines can understand. We also lack a comprehensive set of mechanisms for scrutinizing the behavior of governing algorithms against precise expectations&#8221;</em><BR>[<a href="https://medium.com/mit-media-lab/society-in-the-loop-54ffd71cd802#.o2fyuptpo" target="_blank">https://medium.com/mit-media-lab/society-in-the-loop-54ffd71cd802#.o2fyuptpo</a>]</p>
<p>The Black Sheep Problem: analysing english texts, &#8220;black sheep&#8221; outnumbers &#8220;white sheep&#8221; by 25:1.<BR>[<a href="http://nlpers.blogspot.co.uk/2016/06/language-bias-and-black-sheep.html?m=1" target="_blank">http://nlpers.blogspot.co.uk/2016/06/language-bias-and-black-sheep.html?m=1</a>]</p>
<p>Is already a winner, simply based on its name, but is a warning for automated classification projects&#8230;<BR>[<a href="https://en.m.wikipedia.org/wiki/Ugly_duckling_theorem" target="_blank">https://en.m.wikipedia.org/wiki/Ugly_duckling_theorem</a>]</p>
<p>And not forgetting: the &#8216;uncanny valley&#8217;, when increasingly life-like robots go from cute to creepy<BR>[<a href="https://en.wikipedia.org/wiki/Uncanny_valley" target="_blank">https://en.wikipedia.org/wiki/Uncanny_valley</a>]</p>
<h2>Wardley Mapping</h2>
<p>A nice and simple approach for creating a snapshot view of your current political and system architecture, and getting a sense of where it is (or is not) going.</p>
<p>&#8220;So it’s not just agile we are rebelling against, it’s the tyranny of misapplied doctrine.&#8221;<BR>[<a href="https://medium.com/code-for-america/the-tyranny-of-agile-4e406c1da7fa#.2n5bob1up" target="_blank">https://medium.com/code-for-america/the-tyranny-of-agile-4e406c1da7fa#.2n5bob1up</a>]</p>
<p>And a detailed look at the history and practice of Wardley Mapping by&#8230; Wardley himself.<BR>[<a href="https://medium.com/wardleymaps/on-being-lost-2ef5f05eb1ec#.lnxbl9czk
" target="_blank">https://medium.com/wardleymaps/on-being-lost-2ef5f05eb1ec#.lnxbl9czk</a>]</p>
<h2>Assorted</h2>
<p>Blockchain is not about identity<BR>[<a href="http://thefinanser.com/2016/08/applying-blockchain-identity.html/" target="_blank">http://thefinanser.com/2016/08/applying-blockchain-identity.html/</a>]</p>
<p>Spinning LED displays: [<a href="http://s-m-l.org/" target="_blank">http://s-m-l.org/</a>]<BR>(but probably wasted on birds: [<a href="http://io9.gizmodo.com/5881551/why-birds-dont-like-to-watch-movies" target="_blank">http://io9.gizmodo.com/5881551/why-birds-dont-like-to-watch-movies</a>])</p>
<p><em>&#8220;Combining asset, environmental &#038; now volumetric VR, content (the capture of 4D human performances), we are well on our way to a very new unknown, a complete reinvention of story telling, education experiences &#038; other unique media consumption opportunists&#8221;</em><BR>[<a href="https://vrperception.com/2016/08/16/the-making-of-photogrammetry-to-vr-teaser-of-saint-matthew-in-the-city-aucklandnz/" target="_blank">https://vrperception.com/2016/08/16/the-making-of-photogrammetry-to-vr-teaser-of-saint-matthew-in-the-city-aucklandnz/</a>]</p>
<p>Andrew Betts calling for anyone interested in contributing some thought about paywalls<BR>[<a href="https://twitter.com/triblondon/status/780056954550915072" target="_blank">https://twitter.com/triblondon/status/780056954550915072</a>]</p>
]]></content:encoded>
			</item>
		<item>
		<title>Recently Interesting Links &#8211; August 2016</title>
		<link>http://labs.ft.com/2016/08/recently-interesting-links-august-2016/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=recently-interesting-links-august-2016</link>
		<pubDate>Wed, 24 Aug 2016 11:23:39 +0000</pubDate>
		<dc:creator><![CDATA[FT Labs]]></dc:creator>
				<category><![CDATA[InterestingLinks]]></category>
		<category><![CDATA[Labs]]></category>

		<guid isPermaLink="false">http://labs.ft.com/?p=14229</guid>
		<description><![CDATA[An incomplete and eclectic list of recently interesting items (for some definition of recent, and some definition of interesting)]]></description>
				<content:encoded><![CDATA[<p><em>An incomplete and eclectic list of recently interesting items (for some definition of recent, and some definition of interesting)</em>:</p>
<p><a href="https://www.ft.com/content/e460ba78-5379-11e6-9664-e0bdc13c3bef" target="_blank"><img src="https://next-geebee.ft.com/image/v1/images/raw/http%3A%2F%2Fcom.ft.imagepublish.prod.s3.amazonaws.com%2F8a0b39e8-53a8-11e6-9664-e0bdc13c3bef?source=next&#038;fit=scale-down&#038;width=700" alt="from How do you sift through 794 economic blog posts in a morning?" /></a></p>
<p>&#8220;How do you sift through 794 economic blog posts in a morning? Giles Wilkes navigates the squawking heads&#8221; [<a href="https://www.ft.com/content/e460ba78-5379-11e6-9664-e0bdc13c3bef" target="_blank">How I learnt to love the economic blogosphere</a> &#8211; FT article]</p>
<p>&#8220;we propose and investigate a novel end-to-end method for automatically generating short email responses, called Smart Reply. It generates semantically diverse suggestions that can be used as complete email responses with just one tap on mobile.&#8221; [<a href="https://research.google.com/pubs/pub45189.html"  target="_blank">https://research.google.com/pubs/pub45189.html</a>]</p>
<p>&#8220;a small group of readers could not reliably discern whether a sports article was written by a human or a bot. Those assigned the automated article found it trustworthy and informative, albeit a bit boring.&#8221; [<a href="http://mediashift.org/2016/07/upsides-downsides-automated-robot-journalism/" target="_blank">http://mediashift.org/2016/07/upsides-downsides-automated-robot-journalism/</a>]</p>
<p>&#8220;Hamilton can flag a point in the data &#8211; for instance, a sub-optimal corner or a harsh-sounding gear change, for later analysis.&#8221; [<a href="http://www.wired.co.uk/article/lewis-hamilton-f1-mercedes-wheel" target="_blank">http://www.wired.co.uk/article/lewis-hamilton-f1-mercedes-wheel</a>]</p>
<p>A slant on AR (that it is more likely to work, grab attention a la Pokemon, rather than VR) [<a href="https://vimeo.com/138986112" target="_blank">Beau Lotto – Understanding Perception: How We Experience the Meaning We Create</a> &#8211; Vimeo]</p>
<p>A longer read: the AI future is coming and it doesn&#8217;t really need humans in the loop [<a href="http://www.cringely.com/2016/07/11/thinking-big-data-part-3-final-part/" target="_blank">http://www.cringely.com/2016/07/11/thinking-big-data-part-3-final-part/</a>]</p>
<p>A masterclass of document writing by Bret Victor in 2012. &#8220;Here&#8217;s a trick question: How do we get people to understand programming?&#8221; [<a href="http://worrydream.com/LearnableProgramming/" target="_blank">http://worrydream.com/LearnableProgramming/</a>]</p>
<p>Tim Harford on wishful thinking: &#8220;We are quite capable of clinging on to our beliefs by picking whatever facts support them&#8221; [<a href="https://next.ft.com/content/e8793d78-4880-11e6-8d68-72e9211e86ab" target="_blank">Brexit and the power of wishful thinking</a> &#8211; FT article]</p>
<p>&#8220;As history has hit the fast-forward button, it seems to have become the fashion among philanthropists to endow research institutes that focus on the existential challenges of our age, and this is one of the most remarkable.&#8221; [<a href="https://next.ft.com/content/46d12e7c-4948-11e6-b387-64ab0a67014c" target="_blank">Artificial intelligence: can we control it?</a> &#8211; FT article]</p>
<p>&#8220;Over apple fizz and celeriac soup in London, the sage of ‘superforecasting’ talks about the Brexit curveball and why political pundits get it wrong&#8221; [<a href="https://next.ft.com/content/803a430a-442b-11e6-b22f-79eb4891c97d" target="_blank">Lunch with the FT: Philip Tetlock</a> &#8211; FT article]</p>
<p>&#8220;Today, one can date ‘mobile’ to before iPhone and after iPhone.  But the interesting thing, looking back, is that before the iPhone, it didn’t really feel like we were desperately in need of some catalytic event.&#8221; [<a href="http://ben-evans.com/benedictevans/2016/2/19/mobile-smartphones-and-hindsight" target="_blank">http://ben-evans.com/benedictevans/2016/2/19/mobile-smartphones-and-hindsight</a>]</p>
<p>A (not all that optimistic) concept video of AR++ [<a href="https://vimeo.com/166807261" target="_blank">HYPER-REALITY</a> &#8211; Vimeo]</p>
<p>Liking this interactive graphic &#8211; &#8220;But how does [Mo Farah&#8217;s] pace compare to other Olympic athletes or notable sprinters from the natural world?&#8221; [<a href="https://ig.ft.com/mo-farah-races-a-t-rex/" target="_blank">https://ig.ft.com/mo-farah-races-a-t-rex/</a>]</p>
]]></content:encoded>
			</item>
		<item>
		<title>Finding Hidden Haiku</title>
		<link>http://labs.ft.com/2016/07/finding-hidden-haiku/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=finding-hidden-haiku</link>
		<pubDate>Tue, 12 Jul 2016 16:00:25 +0000</pubDate>
		<dc:creator><![CDATA[FT Labs]]></dc:creator>
				<category><![CDATA[Labs]]></category>

		<guid isPermaLink="false">http://labs.ft.com/?p=14219</guid>
		<description><![CDATA[There are accidental haiku sitting in plain sight but unrecognised in FT articles, until now...]]></description>
				<content:encoded><![CDATA[<p><a href="http://www.ft.com/hidden-haiku"><img class="size-full wp-image-14221" src="http://labs.ft.com/wp-content/uploads/2016/06/hidden-haiku-basic-logo.png" alt="hidden-haiku-basic-logo" width="120" /></a></p>
<p>Somewhere in this extract of an <a href="http://www.ft.com/cms/s/0/146eea98-1539-11e6-b8d5-4c1fcdbe169f.html" target="_blank">article</a> by John Paul Rathbone lurks a haiku. Not placed there deliberately, it, and others like it, have been sitting in plain sight, unrecognised for what they are.</p>
<blockquote><p>Lastly, Mr Temer needs to build a Congressional majority. Although Mr Temer is a wily negotiator, this is perhaps his hardest task — especially as the Petrobras corruption scandal has fractured Congress into myriad whirlpools of seething factions, not only between parties but within them too. Mr Temer has already had to shelve earlier plans to reduce the number of ministers because such appointments are a traditional way of dealing out pork and thus building coalitions.</p></blockquote>
<p>… and here it is:</p>
<ul>
<li style="list-style-type: none; font-size: larger;"><em>has fractured Congress<br />
into myriad whirlpools<br />
of seething factions</em></li>
</ul>
<p>There are plenty more such haiku: identified by computer algorithm, selected by a human, and brought to light in <a href="http://ft.com/hidden-haiku" target="_blank">ft.com/hidden-haiku</a>.</p>
<p><span id="more-14219"></span></p>
<h2>What is a haiku?</h2>
<p>As <a href="https://en.wikipedia.org/wiki/Haiku_in_English" target="_blank">Wikipedia</a> describes, “A haiku in English is a very short poem in the English language, following to a greater or lesser extent the form and style of the Japanese haiku”, sometimes having “a three-line format with 17 syllables arranged in a 5–7–5 pattern”.</p>
<p>For the purposes of this project, we have concentrated more on finding this 5-7-5 syllable structure, and less on “a focus on some aspect of nature or the seasons” and the other, more subtle, criteria. That being said, some of the haiku we have found have come pleasingly close to being powerful pieces of prose in their own right.</p>
<h2>Recognising haiku</h2>
<p>Inspired by the lovely <a href="http://haiku.nytimes.com/about" target="_blank">New York Times Haiku project</a>, we conducted a series of mini-experiments, looking at our content in new ways, exploring a variety of other avenues for the manipulation of search results and the automated identification and manipulation of <a href="#awfulpoetry">‘accidental’ poetry*</a>. Along the way, perhaps inevitably, the tool we built was easily tweaked to become a haiku detector, and it seemed wrong not to point it at the FT articles to see what we could find. A conversation with Jacob Harris (<a href="https://twitter.com/harrisj" target="_blank">@harrisj</a>), formerly of the NYT, concluded that we’ve ended up in a similar haiku place.</p>
<p>The code for our haiku detector is <a href="https://github.com/railsagainstignorance/alignment" target="_blank">available</a> but comes with plenty of caveats, chief among which is that it was My First Golang Program (™). The tool itself is <a href="http://ftlabs-alignment.herokuapp.com/" target="_blank">live</a> but, at the time of writing, the part involving haiku is restricted to FT Staff accounts.</p>
<p>On startup, the haiku detector reads in the <a href="https://raw.githubusercontent.com/railsagainstignorance/alignment/master/rhyme/cmudict-0.7b" target="_blank">134K defined words</a> from the enormous and enormously useful phoneme dataset, <a href="http://www.speech.cs.cmu.edu/cgi-bin/cmudict" target="_blank">CMU Pronouncing Dictionary</a>, and some extra items added as part of this project (more details below).</p>
<p>An example defined word is: <em>HAIKU: HH AY1 K UW0</em>. There are two numbered phonemes, <em>AY1</em> and <em>UW0</em>, so the word has two syllables, with the primary emphasis on the phoneme marked with a 1. Longer words can have a syllable with a secondary emphasis, e.g. <em>DEFENESTRATION: D IY0 F EH2 N EH0 S T R EY1 SH AH0 N</em>.</p>
<p>A word’s phonemes are then mapped to a string of emphasis points. In the case of the example word <em>HAIKU</em>, this would be “10”, indicating this is a two syllable word with emphasis on the first syllable.</p>
<p>The extra items added to the dictionary include</p>
<ul>
<li>straightforward definitions, such as BITCOIN and EUROSCEPTIC, which were missing from the original</li>
<li>alternate (mis)spellings, such as CRITICISED → CRITICIZED</li>
</ul>
<p>and extra functionality</p>
<ul>
<li>regexes for word boundaries, such as /\w+[&#8216;’][sStdmM]/</li>
<li>regexes for transforming awkward text, such as apostrophes, /(\w+)’([sStTdDmM])/$1&#8217;$2/</li>
<li>marking certain words as being unsuited to terminating a phrase in poetry, such as AND and BUT</li>
</ul>
<p>The user specifies the structure of the haiku, “&#8230;.. ……. …..”, representing contiguous blocks of 5+7+5 syllables, with no particular required emphasis, but an individual word cannot be split across two blocks of syllables. This structure, aka <a href="https://en.wikipedia.org/wiki/Metre_(poetry)" target="_blank">meter</a>, is turned into a regular expression to be used to match against concatenations of strings of emphasis points.</p>
<p>The user specifies some articles, such as those currently on the <a href="http://www.ft.com/" target="_blank">FT.com homepage</a> (usually 15), or the <a href="http://www.ft.com/news-feed" target="_blank">most recently published</a> articles, or <a href="http://www.ft.com/comment/lucy-kellaway" target="_blank">Lucy Kellaway’s latest thoughts</a>. The content of each article is pulled in via our <a href="https://developer.ft.com/" target="_blank">Search and Content APIs</a>, and split into words. Each word is looked up in the dictionary to obtain its emphasis string, which defaults to “?” if there is no matching definition and ensures this word will not be a candidate to match the meter regex. The emphasis strings for all the words in the article are concatenated into a space-separated string and the meter regex is applied to look for matching sequences of emphasis points. The matches are unpacked to give the original article text.</p>
<p>The haiku detector lists all the specified articles, and for each one lists the fragments of text which match the haiku meter and do not end with unsuitable words, presented in such a way as to make it easy for the user to visually scan through large numbers of them.</p>
<p>The ‘unsuitable’ haikus are listed at the end, along with all the unrecognised words, such as (at the time of writing) BREWDOG and WHISTLEBLOWING.</p>
<p>Numbers, dates, percentages, etc, could be converted into text, and then be candidates to match within a haiku, but that has been left as an exercise for (maybe, but probably not) later.</p>
<h2>Choosing haiku</h2>
<p>There’s no subtle way of saying it other than, you have to wade through an awful lot of nonsense to get to the good ones. No concrete stats yet, but a haiku hit rate of 1 in a 100 seems about right, i.e. 1 “Hm, maybe” to 100 “No”s.</p>
<p>Returning to John Paul Rathbone’s <a href="http://www.ft.com/cms/s/0/146eea98-1539-11e6-b8d5-4c1fcdbe169f.html" target="_blank">article</a>, here are all the matching, suitable haiku:</p>
<table style="width: 100%; font-style: italic;">
<tbody>
<tr>
<td>week is a supposed<br />
procedural glitch announced<br />
on Monday that could</td>
<td>the lower house will<br />
decide any differently<br />
than it did last month</td>
</tr>
<tr>
<td>faces four daunting<br />
challenges although even<br />
these are not unique</td>
<td>rule Mauricio<br />
Macri Argentina’s new<br />
president faces</td>
</tr>
<tr>
<td>they or candidates<br />
of a similar stature<br />
can inject a dose</td>
<td>private Brazilian<br />
companies can currently<br />
pay to their partners</td>
</tr>
<tr>
<td>Congress including<br />
the head of the lower house<br />
Eduardo Cunha</td>
<td>scandal has fractured<br />
Congress into myriad<br />
whirlpools of seething</td>
</tr>
<tr>
<td>has fractured Congress<br />
into myriad whirlpools<br />
of seething factions</td>
<td>into myriad<br />
whirlpools of seething factions<br />
not only between</td>
</tr>
<tr>
<td>whirlpools of seething<br />
factions not only between<br />
parties but within</td>
<td>of seething factions<br />
not only between parties<br />
but within them too</td>
</tr>
<tr>
<td>here is that in both<br />
countries most citizens care<br />
little for party</td>
<td>reasonably well<br />
run and are willing to give<br />
fresh leaders a chance</td>
</tr>
<tr>
<td>run and are willing<br />
to give fresh leaders a chance<br />
to do that at least</td>
</tr>
</tbody>
</table>
<p>Not shown here, the (perhaps double the number of) matching ‘unsuitable’ haiku.</p>
<p>The selection of haiku from this list of candidates is highly subjective, and done by eye, at speed. No doubt there are still some nice haiku which remain hidden even after this scan.</p>
<h2>Categorising haiku</h2>
<p>It is early days, but over the course of 3 months we have accumulated approximately 300 haiku which pass the “Hm, maybe” test. There are enough to attempt to categorise them into different types, ranging from the mundane to the really rather profound. Here are some of the (overlapping) categories:</p>
<ul>
<li>Cropping changing meaning
<ul>
<li style="list-style-type: none; font-size: larger;"><em>approval ratings<br />
have sunk to new lows prompting<br />
calls for a party</em></p>
<ul>
<li style="list-style-type: none;">By <a href="http://www.ft.com/cms/s/0/9e101508-fc46-11e5-b5f5-070dca6d0a0d.html" target="_blank">Anne-Sylvaine Chassany</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Humour
<ul>
<li style="list-style-type: none; font-size: larger;"><em>The company was<br />
almost forced out of business<br />
due to collapsing</em></p>
<ul>
<li style="list-style-type: none;">By <a href="http://www.ft.com/cms/s/0/b0ede3a6-06c5-11e6-a70d-4e39ac32c284.html" target="_blank">Kana Inagaki and Jennifer Thompson</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Gnomic/profound
<ul>
<li style="list-style-type: none; font-size: larger;"><em>with them to find out<br />
if they will be good or bad<br />
for humanity</em></p>
<ul>
<li style="list-style-type: none;">By <a href="http://www.ft.com/cms/s/2/5a352264-0e26-11e6-ad80-67655613c2d6.html" target="_blank">Richard Waters and Tim Bradshaw</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Imagery
<ul>
<li style="list-style-type: none; font-size: larger;"><em>the two main parties<br />
taking turns to rip themselves<br />
apart in public</em></p>
<ul>
<li style="list-style-type: none;">By <a href="http://www.ft.com/cms/s/0/10a4f714-0ddb-11e6-b41f-0beb7e589515.html" target="_blank">George Parker</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Reportage
<ul>
<li style="list-style-type: none; font-size: larger;"><em>toilet was destroyed<br />
in a controlled explosion<br />
by army experts</em></p>
<ul>
<li style="list-style-type: none;">By <a href="http://www.ft.com/cms/s/2/5588b2be-1abd-11e6-b286-cddde55ca122.html" target="_blank">Andrew Bounds</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Very FT
<ul>
<li style="list-style-type: none; font-size: larger;"><em>Idle capital<br />
is an opportunity<br />
cost for the system</em></p>
<ul>
<li style="list-style-type: none;">By <a href="http://ftalphaville.ft.com/2016/06/01/2164241/the-financial-reserve-vs-real-world-inventory-discrepancy/" target="_blank">Izabella Kaminska</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>Publishing haiku</h2>
<p>We are taking some small, early steps to establish if there is interest among our readers (existing or prospective) in having these haiku brought to their attention, with a weekly collection being published as an article: <a href="http://www.ft.com/hidden-haiku" target="_blank">ft.com/hidden-haiku</a>. These will be tweeted and posted onto Facebook.</p>
<p>Perhaps we will provide an option for our readers to get all their news in haiku form. You heard it here first.</p>
<h2 id="awfulpoetry">* Awful poetry</h2>
<p>Albeit not used in this haiku project, having the details of the syllable emphasis within each word plus the final syllable (e.g. <em>K UW0</em>, sounds like “coo”) gives the raw material from which to auto-generate <a href="http://h2g2.com/approved_entry/A577118" target="_blank">cringingly awful</a>, <a href="https://en.wikipedia.org/wiki/Metre_(poetry)" target="_blank">metered</a>, rhyming poetry. The user can specify other meters, such as <a href="https://en.wikipedia.org/wiki/Iambic_pentameter" target="_blank">iambic pentameter</a>, “0101010101”, and resulting matches are sorted by final syllable.</p>
<p>The reader is spared some examples here, and may have to wait for a followup post.</p>
]]></content:encoded>
			</item>
		<item>
		<title>Performance Widget</title>
		<link>http://labs.ft.com/2016/03/performance-widget/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=performance-widget</link>
		<pubDate>Tue, 22 Mar 2016 10:46:32 +0000</pubDate>
		<dc:creator><![CDATA[FT Labs]]></dc:creator>
				<category><![CDATA[Labs]]></category>

		<guid isPermaLink="false">http://labs.ft.com/?p=14210</guid>
		<description><![CDATA[Helping prioritise performance, accessibility and security aspects of FT products by exposing these usually “invisible” properties in the UI of the websites.]]></description>
				<content:encoded><![CDATA[<p>Fast, secure, accessible websites are used by more users than slower, less secure, inaccessible websites. With this in mind, improving accessibility, performance and security of FT products is something that every stakeholder/product-owner can agree is a good goal to aim for. However, if you view the backlog of work of any user-facing FT website you will see the prioritised work is often not on these properties but is on the more visible aspects such as design, layout, content and functionality. How can we make these, less visible,	 properties become more likely to be prioritised in our backlogs of work? We propose that by revealing these “invisible” features in a widget directly onto the pages viewed by the stakeholders/product-owners will help improve these features in our products and thereby help increase viewership and retention of users. We implemented the widget using a Chrome Extension, and have got a set of stakeholders within the company to install it so we can collect the data to draw a conclusion to the project.</p>
<p><a href="http://labs.ft.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-22-at-10.24.10.png" rel="attachment wp-att-14209"><img alt='a mix of good and bad news' src="http://labs.ft.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-22-at-10.24.10.png" width="408" class="size-full wp-image-14209" srcset="http://labs.ft.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-22-at-10.24.10.png 408w, http://labs.ft.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-22-at-10.24.10-300x282.png 300w" sizes="(max-width: 408px) 100vw, 408px" /></a></p>
<p>Documentation on how members of staff can take part in the project can be found <a href="https://ftlabs-perf-widget.herokuapp.com/">here</a>.<br />
Source code for the project can be found <a href="https://github.com/ftlabs/perf-widget">here</a>.</p>
<p><span id="more-14210"></span></p>
<p>Insights is a Chrome extension which will run a set of synthetic tests on any page visited by the user and place the results from these tests inside a widget on the page written in a contextualised manner to make the results easier to ingest.</p>
<h2>Measuring success of the experiment</h2>
<p>Measuring the success of this experiment will be difficult because other teams may already be looking into these properties (such as Next.ft.com looking into performance, Origami.ft.com at accessibility and the security team at, well, security). Because of the type of outcome we are wanting, i.e. work in these areas to be prioritised,  it is difficult to establish if this work is prioritised. Because of this we decided to use proxy metrics and a bit of surveying. The proxy metrics are gathered on every website visited by the user during the experiment. We archive all the metrics, and at the end of the experiment graph them and see if each or any have improved</p>
<h1>Technical Details</h1>
<h2>Why we chose a Chrome Extension</h2>
<p>One of the NFRs (non-functional requirements) for the application was to make the application simple to deploy and require no day-to-day effort from the users. We had previously discussed three implementation approaches:</p>
<ul>
<li>Origami component added to all the websites involved in the test.</li>
<li>Bookmarklet installed on user’s machines.</li>
<li>Chrome extension installed on user’s machines.</li>
</ul>
<p>Creating an Origami component to be installed on all websites involved in the test would involve co-ordinating efforts with the development teams of those websites and would have also limited our experiment to websites which use Origami and are still under active development.</p>
<p>Installing a bookmarklet on users’ machines would mean that the application can not run automatically, instead requiring the user to manually click the bookmarklet when they were interested in viewing the insights. On top of this, deploying a new version of the bookmarklet would involve each user having to update their bookmarklet. This does not meet our NFR of no day-to-day effort from the users.</p>
<p>A Chrome extension on users’ machines meets all our NFRs, deploying to the internal Chrome Webstore is a very simple procedure that can be automated. Installing the Chrome Extension on our users’ machines is also a very simple, automatable procedure. The extension can also be configured to  run on all the FT-owned websites, where we wanted to gather insights.</p>
<h2>How we gather the insights for a website</h2>
<p>We chose to use synthetic (aka automated) testing as the means for gathering our insights because of concerns about load-handling of live testing. We created a server which has a plugin system for insight providers, making it simple to add new insight providers in the future if the need arises. We have one insight provider for each of the properties we are measuring: </p>
<ul>
<li><a href="https://developers.google.com/speed/pagespeed/insights/">Google PageSpeed</a> for performance,</li>
<li><a href="http://ssllabs.com/ssltest/analyze.html">SSL Labs</a> for security</li>
<li>A custom built contrast-ratio tester for accessibility.</li>
</ul>
<p>Once the insights for a website have been gathered, we store them in a database for historical purposes and to aid in drawing a conclusion against the hypothesis of this experiment. The latest insights for a website are also stored in an in-memory cache to help decrease the response time of the server to API requests.</p>
<h2>How we decide whether an insight is worth highlighting</h2>
<p>In order to help the user decide whether an insight’s result returned by our server is of concern , we place a green tick or a red triangle icon next to each result. The server has a set of predefined range of values for each insight, if an insights value is within this range it is not deemed worth highlighting, if it is outside this range, it is highlighted. An example of this would be the PageSpeed Insights score of a website, we set the range for this insight to be between 60 &#038; 100.</p>
<h2>Testing personalised/authenticated websites</h2>
<p>We chose to use synthetic testing to gather our data to minimise the testing load. One of the issues with synthetic testing is the fact that the testing provider visits the website with their own browser, meaning websites with content personalised for the user would be different on the user’s device compared to the testing provider’s. This is something unavoidable on the web as of today. Another difficulty is websites which require being authenticated. Most insight providers do not provide a way to authenticate websites on their testing devices. One such provider which does is WebPageTest, who have an API to do just that. Because the majority do not, we decided to keep our project simple and would not run any insight providers on websites behind authentication.</p>
]]></content:encoded>
			</item>
		<item>
		<title>GMail Signatures Extension</title>
		<link>http://labs.ft.com/2016/02/gmail-signatures-extension/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=gmail-signatures-extension</link>
		<pubDate>Tue, 23 Feb 2016 11:20:13 +0000</pubDate>
		<dc:creator><![CDATA[FT Labs]]></dc:creator>
				<category><![CDATA[Labs]]></category>

		<guid isPermaLink="false">http://labs.ft.com/?p=14163</guid>
		<description><![CDATA[We appended links to the latest FT articles to make our prospect emails more enticing.]]></description>
				<content:encoded><![CDATA[<p class='standfirst'>Every month, our B2B team send thousands of emails to our clients and prospects across the world. One of the FT&#8217;s goals for 2016 is to reach a target of 1,000,000 subscribers. To that end, we wondered if it was possible to automatically embed links to FT content in all of those emails as they were being sent, with the hope that some recipients would be enticed to subscribe.</p>
<p>With this in mind, we built a Chrome Extension that would detect when a new email was being composed, and would then add in the latest news headlines from an FT RSS feed. Once installed, every new email sent from that FT account would automatically have a tempting signature appended to it looking much like this one below.</p>
<p><figure><img class="alignnone size-medium wp-image-14176" src="http://labs.ft.com/wp-content/uploads/2016/02/GmailSignaturesDemo-1.gif" alt="GmailSignaturesDemo" width="600" /></figure></p>
<p>We&#8217;ve made this extension available to everyone at the FT on our internal Chrome store. If you&#8217;re one of us, you can <a href="https://chrome.google.com/webstore/detail/ftlabs-gmail-signatures/jolenialiljjnmhelekbmpoplemdbjjo">grab it here</a></p>
<p>If you&#8217;re of a technical disposition, you can take a peek at the source code for our <a href="https://github.com/ftlabs/email-signatures">extension</a> and for the <a href="https://github.com/ftlabs/email-signatures-server">service</a> that delivers content to it.</p>
<p><b>Why a Chrome Extension?</b></p>
<p>At the FT we have Google Apps for Work, so a great number of our B2B team use the Gmail Web interface to email our prospects, and the Chome Extensions API gives us great flexibility when it comes to manipulating the content of pages, so it was the natural choice.</p>
<p><b>How does it work?</b></p>
<p>The extension itself is a rather simple affair, once installed, a user can click on the icon in the top right of their screen to reveal a dialog which allows them to configure the source of the articles (by way of a public RSS feed), the number of articles to insert, the theme, and content to insert. Once those settings have been saved, every time a user of the extension starts to write an email we append the signatures, according to the configuration, to the compose dialog.</p>
<p>As mentioned above, we get the content for our signatures from an RSS feed, but the retrieval and parsing of the RSS content doesn&#8217;t happen from within the Extension itself, this happens in a small Node.js app that we&#8217;ve created to work in tandem with the extension. Whenever it&#8217;s time to append a signature to an email, our Chrome extension makes a request to our Node app, passing it the URL and configuration options for the content we want to add to the email. Our server then goes and grabs the RSS feed, parses its content and returns the HTML that gets inserted into the signature to our extension. The extension then appends this content to our emails.</p>
<p><b>How do we know where and when to insert the signature?</b></p>
<p>Inserting content into a webpage with a Chrome extension is a simple task, but knowing when to insert the signature requires some consideration. We don&#8217;t want to risk inserting random content into nodes in the GMail interface, and we don&#8217;t want to perform expensive checks to see when a compose dialog had been opened, as this could adversely affect the user&#8217;s experience. With this in mind, we opted to use the MutationOberver API. This neat API lets us execute code every time a node is created or removed from a page, so instead of checking to see if something has changed on a page, we know for certain that something has. All that needs doing next, is deciding whether or not we need to insert a signature.</p>
<p>With the MutationOberserver, we know when we can insert a signature, but the question of whether we should is yet to be resolved. The simplest solution is to append the signatures to every single email being sent, brand new ones and replies, but after chatting with our B2B compatriots, we felt that this might be overkill. Instead, we decided to only automatically append signatures to brand new emails and to give the users of the extension the ability to manually add signatures to other compose dialogs with a new button</p>
<p><figure><img class="alignnone size-medium wp-image-14179" src="http://labs.ft.com/wp-content/uploads/2016/02/FTLabs-GmailSigs-Just_One.gif" alt="FTLabs-GmailSigs-Just_One_FULL" /></figure></p>
<p>Gmail doesn&#8217;t make it easy to find the right place to put our signatures. Instead of nice, semantic attributes to suggest the function or purpose of each DOM node, a random string identifies each node. We suspect this is a result of Google&#8217;s Closure tool. Fortunately, these remain constant, so once we managed to identify the characters that a compose dialog has, we could then insert our signature into the editable text area of the compose dialog. This works for now, but if the characters we use to identify compose dialogs change, we would need to manually update our extension to handle the new identifiers. </p>
<p>While we append the signatures to the emails, we also set some custom data attributes, so we can identify compose dialogs which have already had the signature appended to the emails. This allows us to update signatures that have already been appended to emails and stops us from overwriting existing signatures that may have been modified by the user.</p>
<p><b>How will we measure the success of this project?</b></p>
<p>At the time of writing, most of our London B2B team have the signatures Chrome extension installed on their systems. We&#8217;re currently tracking clickthroughs and a variety of other metrics to determine whether the recipients of emails with signatures are clicking through to FT content. If, after a certain time, we can see an increase in subscriptions following a click on one of our emails, we&#8217;ll consider the project a success.</p>
]]></content:encoded>
			</item>
	</channel>
</rss>
